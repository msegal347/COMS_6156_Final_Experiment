{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the SHAP documentation.\n",
    "\n",
    "https://shap-lrjball.readthedocs.io/en/latest/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import shap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, AutoConfig\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "# Load the tokenizer and model with the token\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", use_auth_token=hf_token)\n",
    "model = BertForQuestionAnswering.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", use_auth_token=hf_token)\n",
    "\n",
    "# If you are using the model for inference, place it in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example context and question\n",
    "context = \"Hugging Face is a company based in New York City. Its technology is widely used in the industry.\"\n",
    "question = \"Where is Hugging Face based?\"\n",
    "\n",
    "# Encode inputs\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the score\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# Convert ids to tokens and join them to get the answer\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end])\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#hf_token = \"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\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# Define the prediction function for question answering\n",
    "def predict(inputs):\n",
    "    # Tokenize inputs while padding and maintaining equal length\n",
    "    encoded = [tokenizer.encode_plus(input_text, add_special_tokens=True, max_length=512, pad_to_max_length=True, return_tensors=\"pt\") for input_text in inputs]\n",
    "    input_ids = torch.cat([e[\"input_ids\"] for e in encoded]).cuda()\n",
    "    attention_mask = torch.cat([e[\"attention_mask\"] for e in encoded]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    # Select start logits and convert to probabilities\n",
    "    start_logits = outputs.start_logits\n",
    "    start_probs = torch.nn.functional.softmax(start_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Convert to a simpler score (e.g., the logit of the maximum probability)\n",
    "    max_probs = np.max(start_probs, axis=-1)\n",
    "    logit_scores = sp.special.logit(max_probs)  # Convert probabilities to logit scores\n",
    "    return logit_scores\n",
    "\n",
    "# Build an explainer using a token masker\n",
    "explainer = shap.Explainer(predict, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions and contexts\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Romeo and Juliet?\"\n",
    "]\n",
    "contexts = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Romeo and Juliet is a play written by William Shakespeare.\"\n",
    "]\n",
    "\n",
    "inputs = [q + \" [SEP] \" + c for q, c in zip(questions, contexts)]\n",
    "\n",
    "# Explain the model's predictions on these inputs\n",
    "shap_values = explainer(inputs)\n",
    "\n",
    "# Plot the explanation for the first input\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artificial_intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
